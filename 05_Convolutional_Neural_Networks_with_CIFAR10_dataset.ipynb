{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Sample Work: CNN for CIFAR10 in Keras</h1></center>\n",
    "\n",
    "This notebook consists of a Convolutional Neural Network written in Keras, trained on the CIFAR10 dataset.  I'll add some basic information at each step to cover all the salient points of the model/dataset/process. \n",
    "\n",
    "\n",
    "<center><h3>The Dataset</h3></center>\n",
    "\n",
    "**_CIFAR10_** is a subset of the Tiny Images dataset containing 10 classes of images labeled images.  All images are 32 x 32 x 3 (meaning the image is in color, as opposed to black and white). There are a total of 60000 images in the dataset, meaning 6000 images per class. This is considered a harder image classification challenge than an introductory dataset such as MNIST because of the increased dimensionality that comes with 3 color channels and a slightly larger size.  \n",
    "\n",
    "\n",
    "<center><h3>Tools Used</h3></center>\n",
    "\n",
    "I chose to build this in **_Keras_** because the Keras API makes it incredibly easy to build prototypes.  With Keras, we get all of the benefits of Tensorflow under the hood, but without the boilerplate and overhead of pure Tensorflow (As of TF 1.5, Eager Execution is now available as a feature--I suspect that as this gains traction, complaints about Tensorflow boilerplate and setup overhead will become a thing of the past).  Keras contains great open-source implementations of all the layers needed to solve this problem, which allows us to define the architecture of our CNN with only one or two lines per layer.  \n",
    "\n",
    "As an added benefit, Keras and Tensorflow both include _CIFAR10_ as one of their pre-packaged datasets for benchmarking model performance, so getting our data is also easy.  Aside from Keras, the only other tool used is Matplotlib, which is only used once for visualizing what an instance of our dataset actually looks like. \n",
    "\n",
    "\n",
    "<center><h3>Model Architecture</h3></center>\n",
    "\n",
    "For this model, I chose to use a Convolutional Neural Network.  Here's a little bit about each layer used in the model, and what they are used for:\n",
    "\n",
    "\n",
    "**_Convolutional Layers_**--Convolutional layers are the workhorse of CNNs, which explains the name. Convolutional Neural Networks are the considered the classic approach to image classification.  Although it's technically possible to build an image classifier using only Dense layers, a traditional Deep Neural Network would take orders of magnitude more data without the benefits of the feature maps created by the Convolutional Layers.  The main benefit of convolutional layers is that they reduce the dimensionality of of the data while mostly preserving the important information.  In this model, I use 4 Convolutional layers, with a MaxPooling2d layer after every 2 Conv2D layers.  \n",
    "\n",
    "**_MaxPooling Layers_**--Pooling Layers are generally always used after Convolutional Layers to further reduce dimensionality.  MaxPooling is generally used, but AvgPooling is an option.  This layer looks at the feature map output by the layer before it in 2 x 2 blocks, and replaces the 2 x 2 block with the highest value contained within the block.  For instance, a MaxPooling2D layer would replace the 2x2 window `[[10, 7],[-4, 3]]` with the value `10`.  \n",
    "\n",
    "**_Dense_**-- Dense layers are the bread and butter of Neural Networks.  These are just regular fully-connected layers, where the inputs for every neuron are the outputs from every neuron in the previous layer, along with a vector of weights corresponding to how heavily the neuron weights each output from the neuron that sent it.  In practice, these are just vectora of weights which correspond to a vector of outputs, to make the math simpler.  The neuron computes the dot product of the weight vector and output vector, adds a bias (this is randomly initialized and learned, just like the weights), and then feeds this value (usually denoted as _z_) through the chosen activation function.  The output of the activation function is then passed as input to every neuron in the following layer.\n",
    "\n",
    "**_Activation_**-- This adds an activation function to feed the data through.  In computer vision, **_Rectified Linear Units (ReLU)_** are--by far--the most effective activation functions. ReLU outputs 0 for any negative value, but leaves any non-negative values alone.  See the graph below for a visual explanation.  \n",
    "\n",
    "\n",
    "In the last layer, we use a **_Softmax_** activation function.  This function outputs a vector the shape of our labels, with each value in the vector corresponding to the percentage chance the model gives the data of being the corresponding class at that label.  The model then classifies the data as the class in the softmax output with the highest probability value.  Importantly, the sum of all values in a vector output by a softmax function will always be 1.  \n",
    "<br>\n",
    "<center>Example Softmax Output: `[0.02, 0.0017, 0.96, ..... 0.001]`</center>\n",
    "\n",
    "<center>Example Label Mappings: `[\"Dog\", \"Truck\", \"Airplane\",....... \"Bird\"]`</center>\n",
    "\n",
    "**_Flatten_**: This one is simple--it's just Keras's verison of `numpy.reshape`.  This reshapes n-dimensional arrays to a vector.  This is necessary when moving from Conv2D layers, which expect 2-dimensional arrays as inputs, to Dense layers, which expect 1-dimension vectors as inputs.  As a concrete example, a Flatten layer given a 28 x 28 array as input would output a vector of the shape (784, 1).  \n",
    "\n",
    "**_Dropout_**: Dropout layers are an effective form of regularization used in Deep Learning to prevent overfitting.  During training, the dropout layer \"turns off\" neurons in the previous layer, with the percentage chance of a neuron turning off being a tuneable hyperparameter passed in when the layer is created.  In this model, I've set a 25% dropout chance for neurons after the MaxPooling2D layers, and a 50% dropout chance after the Dense layers.  For a good intuition behind why dropout is effective, picture a sports team where one player is a superstar, and the coach's only objective is in winning. If the coach always relies on the superstar to do all the work, the other players (neurons) won't get much practice, and so won't learn to become better.  With dropout, there's a 50% chance the superstar neurons are turned off, giving the other neurons more practice (and more opportunities to make mistakes, which means more weight correction during the back propagation step).  Dropout layers are only used during the training step--they are turned off during testing.  \n",
    "\n",
    "<center><h1>Walking Through the Code</h1></center>\n",
    "\n",
    "We'll start by importing everything that we'll be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten\n",
    "from keras.layers import MaxPooling2D, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll **declare important constants** such as the number of classes, batch size, etc.  We'll also create our callback which allows us to use Tensorboard to inspect all the logs from training our model.  \n",
    "\n",
    "In the cell below, we'll import the data and split it into training and testing sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare hyperparameters and Constants\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 5\n",
    "NUM_PREDICTIONS = 10\n",
    "BATCH_SIZE = 128\n",
    "tf_callback = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we'll **normalize our image data**.  Since we're working with images, we don't need to use the normal methods for normalizing data. Since all pixels in an image can only be a value between 0 and 255, we'll just divide everything by 255.  We'll also convert our data from integers to floats.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare images\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll  **one-hot encode our labels**.  This means we'll turn every label into a sparse vector where all values are zero, except at the index corresponding to the correct label.  For instance, if the indexes of our labels mapped to `['dog', 'cat', 'truck']` and the first image in the dataset is of a cat, then we could expect the one-hot encoded label for the first image to look like `[0, 1, 0]`. \n",
    "\n",
    "Below that, I've taken the optional step of displaying a random image from the dataset just to see what we're actually working with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot vectors\n",
    "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x184e0646390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x184d64018d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's peek at a sample image to see what one looks like:\n",
    "plt.imshow(X_train[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **build the model layer by layer**. `Sequential()` objects are the base models in Keras, which we then add layers to.  The Convolutional layers learn 32 image filters, with a window size of 3 x 3.  `padding=same` tells the model that the feature maps output by the convolutional layers should be the same dimensionality as the input. This is accomplished by padding the image with extra pixels with values of 0.  \n",
    "\n",
    "To declare the size of the input layer, we just grab it from `X_train.shape`.  However, we don't include the first element, because that number corresponds to how many images are in our training set, not the shape of each image.  This means that we're telling the model to create an input layer for data of shape `(32, 32, 3)`--32 pixels high by 32 pixels wide, with 3 color channels (red, green, and blue). \n",
    "\n",
    "Once we've finished adding all the layers we want to our model object, we then compile it, and (optionally, but recommended) inspect a summary of what we've built.  At the bottom of the summary, we can see that this model has 1,411,242 different trainable parameters--these are the weights and biases that the model will initialize with random values to start, but will learn the correct values for during back propagation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,411,242\n",
      "Trainable params: 1,411,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES,  activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we **fit the model**. During this step, we pass it our training data, as well as the values we previously declared for batch size, number of epochs, etc.  We can also save ourselves some time by passing it the validation data (X_test and y_test) at this step as well.  Finally, we pass in the Tensorboard callback we set above as an optional value for `callbacks`, so that the model will write logs for us that we can inspect in Tensorboard.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 15s 302us/step - loss: 0.9184 - acc: 0.6838 - val_loss: 0.7897 - val_acc: 0.7287\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.8652 - acc: 0.7038 - val_loss: 0.7644 - val_acc: 0.7391\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.8257 - acc: 0.7201 - val_loss: 0.7468 - val_acc: 0.7474\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 15s 303us/step - loss: 0.7866 - acc: 0.7307 - val_loss: 0.7281 - val_acc: 0.7527\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 15s 303us/step - loss: 0.7697 - acc: 0.7391 - val_loss: 0.7105 - val_acc: 0.7554\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 15s 303us/step - loss: 0.7324 - acc: 0.7531 - val_loss: 0.6913 - val_acc: 0.7643\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 15s 302us/step - loss: 0.7128 - acc: 0.7593 - val_loss: 0.6727 - val_acc: 0.7760\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 15s 302us/step - loss: 0.6898 - acc: 0.7669 - val_loss: 0.6771 - val_acc: 0.7705\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 15s 303us/step - loss: 0.6687 - acc: 0.7722 - val_loss: 0.6656 - val_acc: 0.7741\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.6500 - acc: 0.7785 - val_loss: 0.6620 - val_acc: 0.7733\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 0.6331 - acc: 0.7861 - val_loss: 0.6458 - val_acc: 0.7817\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 15s 299us/step - loss: 0.6187 - acc: 0.7912 - val_loss: 0.6479 - val_acc: 0.7808\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 15s 302us/step - loss: 0.6043 - acc: 0.7943 - val_loss: 0.6417 - val_acc: 0.7811\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 15s 303us/step - loss: 0.5943 - acc: 0.7984 - val_loss: 0.6384 - val_acc: 0.7870\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 0.5759 - acc: 0.8052 - val_loss: 0.6376 - val_acc: 0.7848\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.5692 - acc: 0.8066 - val_loss: 0.6467 - val_acc: 0.7814\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 15s 302us/step - loss: 0.5538 - acc: 0.8139 - val_loss: 0.6505 - val_acc: 0.7847\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.5470 - acc: 0.8170 - val_loss: 0.6243 - val_acc: 0.7894\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 15s 302us/step - loss: 0.5342 - acc: 0.8200 - val_loss: 0.6217 - val_acc: 0.7915\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 15s 303us/step - loss: 0.5297 - acc: 0.8210 - val_loss: 0.6394 - val_acc: 0.7869\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.5295 - acc: 0.8228 - val_loss: 0.6325 - val_acc: 0.7938\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 0.5130 - acc: 0.8284 - val_loss: 0.6280 - val_acc: 0.7963\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.5018 - acc: 0.8322 - val_loss: 0.6353 - val_acc: 0.7885\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.5035 - acc: 0.8320 - val_loss: 0.6241 - val_acc: 0.7917\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 0.4881 - acc: 0.8361 - val_loss: 0.6296 - val_acc: 0.7915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x185295ed6a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=25, \n",
    "         validation_data=(X_test, y_test), shuffle=True, callbacks=[tf_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After 5 epochs, we can see that the model has an accuracy score on the validation set 70.69%.  The model is clearly not done learning.  With a solid Nvidia GPU or access to a p2.xlarge instance on Amazon EC2, I would probably run this with at least 100 epochs to see where the model converges.  However, in the instance of keepings things fast, free, and still runnable on computers without access to a solid GPU, I've artifically capped the epochs at 5.  \n",
    "\n",
    "The next steps for this model would be tuning the hyperparameters.  The first questions I would likely explore would be things such as:\n",
    "\n",
    "--What effect does a larger or smaller learning rate have on model convergence?\n",
    "--What are the trade offs between run time and learning based on larger batch sizes?\n",
    "--How does a wider or deeper model architecture compare to this one in terms of run time and overall performance?\n",
    "\n",
    "If you have any further questions about my implementation of this model, I'm happy to answer them.  Please email me at _guido.vanrawesome@gmail.com._\n",
    "\n",
    "--Mike Kane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
