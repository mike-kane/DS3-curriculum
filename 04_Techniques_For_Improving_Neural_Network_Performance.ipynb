{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=60% width=60%></center>\n",
    "\n",
    "\n",
    "<center><h1>Techniques For Improving Neural Network Performance</h1></center>\n",
    "\n",
    "In the first three notebooks, we learned about the basics of building a deep neural network, and a litte bit about the basic architecture.  We learned about the history of Single-Layer Perceptrons, what each neuron actually calculates, the different activation functions we can use to capture non-linearity in our model, and finally about the almight backprop algorithm that allowed our Multilayer Perceptron to \"learn\" the proper values for every parameter in the model, even though all those weights and biases started out as random numbers.\n",
    "\n",
    "In this notebook, we'll learn about a few different small-but-handy tricks that can make our models more more accurate, faster to train, and more resilient to overfitting.  Let's briefly examine the topics we'll cover in this notebook:\n",
    "\n",
    "<center><h3>1. Dropout Layers</h3></center>\n",
    "\n",
    "**_Dropout_** is a **_regularization technique_** designed to help prevent the model from **_overfitting_**.  It works by giving every neuron in a given layer a dropout percentage, which is the percentage chance that the neuron will be turned off rather than passing along it's input.  In this lesson, we'll learn about how this technique forces our neural networks to learn to be more resilient by probabilistically \"killing off\" neurons during training.\n",
    "\n",
    "<center><h3>2. Batch Normalization</h3></center>\n",
    "\n",
    "**_Batch Normalization_** (Batch Norm, for short) is a technique that is underpinned by a concept you're already familiar with--**_normalizing_** our data.  As we learned in DS2, models do better when the data is normalized (transformed to a mean of 0 with unit variance).  We already use best practices to normalize data before it is fed into the input layer, but the data is transformed again and again as it moves through each layer of the network.  This can shift it away from a normalized state, causing the model to take much longer to find the correct fit.  With Batch Normalization layers, we can fix this, normalizing data again _between layers_.\n",
    "\n",
    "<center><h3>3. Hyperparameter Tuning</h3></center>\n",
    "\n",
    "**_Hyperparameter Tuning_** is the hardest of the three topics we'll cover in this notebook.  Just as we used GridSearchCV to find the best combination of parameters for our models in DS2, we'll explore strategies for Hyperparameter Tuning to find the best combination for our Neural Networks in this notebook.  You'll likely find hyperparameter tuning for DS3 much harder and more time consuming than it was in DS2--neural networks have many more hyperparameters, and take longer to train, which means that combinatoric approaches like GridSearch are usually untenable. In this notebook, we'll explore the hyperparameters we should tune, and strategies for how to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
